{"cells":[{"cell_type":"markdown","metadata":{"id":"EOgDUDMAG6mn"},"source":["##### Copyright 2022 The TensorFlow Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3PsBDmGG_W8"},"outputs":[],"source":["                    #@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_QoZmhCDqRw","executionInfo":{"status":"ok","timestamp":1697460636235,"user_tz":-480,"elapsed":24068,"user":{"displayName":"gaya wood","userId":"07038287642761661483"}},"outputId":"39109429-3815-4132-85ba-dc7ccd8befd2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"m2o5G7bwJ7d0"}},{"cell_type":"markdown","metadata":{"id":"ifkGYxdCHIof"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"code","source":["# 数据预处理，深度和RGB数据为右边320*240RGB数据\n","import os\n","import numpy as np\n","import cv2\n","# 创建保存视频的目录，深度和RGB数据为右边320*240RGB数据\n","# if not os.path.exists('./drive/MyDrive/dataFall/notFall'):\n","#    os.makedirs('./drive/MyDrive/dataFall/notFall')\n","# 视频路径\n","# input_folder_path = './drive/MyDrive/originalDataFall/deep+rgbNotFall'  # 替换为你的视频文件路径\n","# output_folder_path = './drive/MyDrive/dataFall/notFall'  # 输出视频路径\n","\n","if not os.path.exists('./drive/MyDrive/dataFall/Fall'):\n","    os.makedirs('./drive/MyDrive/dataFall/Fall')\n","# 视频路径\n","input_folder_path = './drive/MyDrive/originalDataFall/gaigaiFall'  # 替换为你的视频文件路径\n","output_folder_path = './drive/MyDrive/dataFall/Fall'  # 输出视频路径\n","\n","# 把480*360或者270的视频，转化为notFall 320*240的视频\n","# if not os.path.exists('./drive/MyDrive/dataFall/notFall'):\n","#    os.makedirs('./drive/MyDrive/dataFall/notFall')\n","# 视频路径\n","# input_folder_path = './drive/MyDrive/originalDataFall/rgbNotFall'  # 替换为你的视频文件路径\n","# output_folder_path = './drive/MyDrive/dataFall/notFall'  # 输出视频路径"],"metadata":{"id":"ZwZ9aA4kKPBb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 遍历文件夹中的所有视频文件，改写视频大小为指定尺寸\n","for filename in os.listdir(input_folder_path):\n","    if filename.endswith(\".mp4\"):  # 你可以添加更多的视频格式，比如 \".avi\", \".mov\" 等\n","        input_video_path = os.path.join(input_folder_path, filename)\n","        output_video_path = os.path.join(output_folder_path, filename)\n","\n","        # 使用opencv打开视频\n","        cap = cv2.VideoCapture(input_video_path)\n","        if not cap.isOpened():\n","            print(f\"Unable to open video: {input_video_path}\")\n","            continue\n","\n","        # 获取视频的fps\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","        # 创建一个VideoWriter对象，用于输出视频\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_video_path, fourcc, fps, (360, 240))\n","\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","\n","            if not ret:\n","                break\n","\n","            # 改变帧的尺寸\n","            resized_frame = cv2.resize(frame, (360, 240))\n","\n","            # 写入输出视频\n","            out.write(resized_frame)\n","\n","        cap.release()\n","        out.release()"],"metadata":{"id":"TZfWjFyqKZ94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# deep+rgbFall遍历文件夹中的所有视频文件，按指定帧大小处理所有视频文件\n","for filename in os.listdir(input_folder_path):\n","    if filename.endswith(\".mp4\"):  # 你可以添加更多的视频格式，比如 \".avi\", \".mov\" 等\n","        input_video_path = os.path.join(input_folder_path, filename)\n","        output_video_path = os.path.join(output_folder_path, filename)\n","\n","        # 使用opencv打开视频\n","        cap = cv2.VideoCapture(input_video_path)\n","        if not cap.isOpened():\n","            print(f\"Unable to open video: {input_video_path}\")\n","            continue\n","\n","        # 获取视频的fps和帧大小\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","        # 确保视频大小是 640x240\n","        assert frame_width == 640 and frame_height == 240, \"Video frame size should be 640x240\"\n","\n","        # 创建一个VideoWriter对象，用于输出视频\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_video_path, fourcc, fps, (320, 240))\n","\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","\n","            if not ret:\n","                break\n","\n","            # 分割帧为两部分\n","            left_frame = frame[:, :320]\n","            right_frame = frame[:, 320:]\n","\n","            # 写入输出视频\n","            out.write(right_frame)\n","\n","        cap.release()\n","        out.release()\n"],"metadata":{"id":"3fyAYDdVKOzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# 盖盖数据从480*854的数据，变成320*240的数据,说明数据格式要统一\n","if not os.path.exists('./drive/MyDrive/dataFall/Fall'):\n","    os.makedirs('./drive/MyDrive/dataFall/Fall')\n","# 视频路径\n","input_folder_path = './drive/MyDrive/originalDataFall/gaigaiFall'  # 替换为你的视频文件路径\n","output_folder_path = './drive/MyDrive/dataFall/Fall'  # 输出视频路径"],"metadata":{"id":"qDSnSTSX6kq4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 把盖盖摔倒视频进行转换，遍历文件夹中的所有视频文件\n","for filename in os.listdir(input_folder_path):\n","    if filename.endswith(\".mp4\"):  # 你可以添加更多的视频格式，比如 \".avi\", \".mov\" 等\n","        input_video_path = os.path.join(input_folder_path, filename)\n","        output_video_path = os.path.join(output_folder_path, filename)\n","\n","        # 使用opencv打开视频\n","        cap = cv2.VideoCapture(input_video_path)\n","        if not cap.isOpened():\n","            print(f\"Unable to open video: {input_video_path}\")\n","            continue\n","\n","        # 获取视频的fps和帧大小\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","        # 确保视频大小是 480x854\n","        assert frame_width == 480 and frame_height == 854, \"Video frame size should be 480x854\"\n","\n","        # 创建一个VideoWriter对象，用于输出视频\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_video_path, fourcc, fps, (320, 240))\n","\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","\n","            if not ret:\n","                break\n","\n","            # 从帧的中心裁剪出320x240的区域\n","            start_x = frame_width // 2 - 160  # 320 / 2 = 160\n","            start_y = frame_height // 2 - 120  # 240 / 2 = 120\n","            cropped_frame = frame[start_y:start_y+240, start_x:start_x+320]\n","\n","            # 写入输出视频\n","            out.write(cropped_frame)\n","\n","        cap.release()\n","        out.release()\n"],"metadata":{"id":"6L3lA-kNurav"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import time\n","\n","def rename_files_by_creation_time(directory_path, prefix):\n","    \"\"\"根据文件的创建时间，对文件夹中的文件进行重命名。\n","\n","    Args:\n","        directory_path (str): 文件夹的路径。\n","        prefix (str): 新文件名的前缀。\n","    \"\"\"\n","    # 获取文件夹中的所有文件\n","    files = os.listdir(directory_path)\n","\n","    # 遍历每一个文件\n","    for filename in files:\n","        # 获取文件的完整路径\n","        file_path = os.path.join(directory_path, filename)\n","\n","        # 检查文件是否为普通文件\n","        if os.path.isfile(file_path):\n","            # 获取文件的创建时间\n","            creation_time = os.path.getctime(file_path)\n","\n","            # 将创建时间转换为字符串形式\n","            creation_time_str = str(int(creation_time))\n","\n","            # 构建新的文件名\n","            new_filename = f\"{prefix}{creation_time_str}_{filename}\"\n","\n","            # 构建新的文件路径\n","            new_file_path = os.path.join(directory_path, new_filename)\n","\n","            # 重命名文件\n","            os.rename(file_path, new_file_path)\n","            print(f\"重命名文件：{filename} -> {new_filename}\")\n"],"metadata":{"id":"1wuMZ6uhpcDH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWxDDkRwLVMC"},"source":["# Transfer learning for video classification with MoViNet\n","\n","MoViNets (Mobile Video Networks) provide a family of efficient video classification models, supporting inference on streaming video. In this tutorial, you will use a pre-trained MoViNet model to classify videos, specifically for an action recognition task, from the [UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). A pre-trained model is a saved network that was previously trained on a larger dataset. You can find more details about MoViNets in the [MoViNets: Mobile Video Networks for Efficient Video Recognition](https://arxiv.org/abs/2103.11511) paper by Kondratyuk, D. et al. (2021). In this tutorial, you will:\n","\n","* Learn how to download a pre-trained MoViNet model\n","* Create a new model using a pre-trained model with a new classifier by freezing the convolutional base of the MoViNet model\n","* Replace the classifier head with the number of labels of a new dataset\n","* Perform transfer learning on the [UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php)\n","\n","The model downloaded in this tutorial is from [official/projects/movinet](https://github.com/tensorflow/models/tree/master/official/projects/movinet). This repository contains a collection of MoViNet models that TF Hub uses in the TensorFlow 2 SavedModel format.\n","\n","This transfer learning tutorial is the third part in a series of TensorFlow video tutorials. Here are the other three tutorials:\n","\n","- [Load video data](https://www.tensorflow.org/tutorials/load_data/video): This tutorial explains much of the code used in this document; in particular, how to preprocess and load data through the `FrameGenerator` class is explained in more detail.\n","- [Build a 3D CNN model for video classification](https://www.tensorflow.org/tutorials/video/video_classification). Note that this tutorial uses a (2+1)D CNN that decomposes the spatial and temporal aspects of 3D data; if you are using volumetric data such as an MRI scan, consider using a 3D CNN instead of a (2+1)D CNN.\n","- [MoViNet for streaming action recognition](https://www.tensorflow.org/hub/tutorials/movinet): Get familiar with the MoViNet models that are available on TF Hub."]},{"cell_type":"markdown","metadata":{"id":"GidiisyXwK--"},"source":["## Setup\n","\n","Begin by installing and importing some necessary libraries, including:\n","[remotezip](https://github.com/gtsystem/python-remotezip) to inspect the contents of a ZIP file, [tqdm](https://github.com/tqdm/tqdm) to use a progress bar, [OpenCV](https://opencv.org/) to process video files (ensure that `opencv-python` and `opencv-python-headless` are the same version), and TensorFlow models ([`tf-models-official`](https://github.com/tensorflow/models/tree/master/official)) to download the pre-trained MoViNet model. The TensorFlow models package are a collection of models that use TensorFlow’s high-level APIs."]},{"cell_type":"code","source":["# 程序正式开始"],"metadata":{"id":"3-TT2Tq5pd0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nubWhqYdwEXD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695033622539,"user_tz":-480,"elapsed":22068,"user":{"displayName":"gaya wood","userId":"07038287642761661483"}},"outputId":"4fe3c3cd-dadd-4367-abb3-2b8082fd3c6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting remotezip\n","  Downloading remotezip-0.12.1.tar.gz (7.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Collecting opencv-python==4.7.0.72\n","  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opencv-python-headless==4.7.0.72\n","  Downloading opencv_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-models-official\n","  Downloading tf_models_official-2.13.2-py2.py3-none-any.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.7.0.72) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from remotezip) (2.31.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from remotezip) (0.9.0)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (3.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (9.4.0)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (0.5.0)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (2.84.0)\n","Collecting immutabledict (from tf-models-official)\n","  Downloading immutabledict-3.0.0-py3-none-any.whl (4.0 kB)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.5.16)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (3.7.1)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (4.1.3)\n","Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.5.3)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (5.9.5)\n","Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (9.0.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (2.0.7)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (6.0.1)\n","Collecting sacrebleu (from tf-models-official)\n","  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.11.2)\n","Collecting sentencepiece (from tf-models-official)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval (from tf-models-official)\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.16.0)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (4.9.2)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (0.14.0)\n","Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official)\n","  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-text~=2.13.0 (from tf-models-official)\n","  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (2.13.0)\n","Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.1.0)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.22.0)\n","Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.17.3)\n","Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.1.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.11.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (2023.7.22)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (8.0.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.0.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (6.0.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official) (2023.3.post1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (1.57.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (3.9.0)\n","Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (2.13.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (16.0.6)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (67.7.2)\n","Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (2.13.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (2.3.0)\n","Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (1.15.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official) (0.33.0)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (3.1.1)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official) (0.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official) (0.3.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official) (4.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (3.4)\n","Collecting portalocker (from sacrebleu->tf-models-official)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official) (2023.6.3)\n","Collecting colorama (from sacrebleu->tf-models-official)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official) (4.9.3)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official) (1.2.2)\n","Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (0.4.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (8.1.7)\n","Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (1.4.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (1.14.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-models-official) (0.41.2)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official) (6.0.1)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official) (3.16.2)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.60.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official) (5.3.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.2.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official) (3.4.4)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official) (2.3.7)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official) (2.1.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official) (3.2.2)\n","Building wheels for collected packages: remotezip, seqeval\n","  Building wheel for remotezip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for remotezip: filename=remotezip-0.12.1-py3-none-any.whl size=7934 sha256=18eeb1942a1a1a4ff9df8770681f176b0de95827b7a7edcf5aaeb3401d368a25\n","  Stored in directory: /root/.cache/pip/wheels/fc/76/04/beed1a6df4eb7430ee13c3900746edd517e5e597298d1f73f3\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=b052a0358e4376289c396a49035b8c924cf8c34e2723a768e0a7495e19dac621\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built remotezip seqeval\n","Installing collected packages: sentencepiece, tensorflow-model-optimization, portalocker, opencv-python-headless, opencv-python, immutabledict, colorama, sacrebleu, remotezip, seqeval, tensorflow-text, tf-models-official\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.8.0.76\n","    Uninstalling opencv-python-headless-4.8.0.76:\n","      Successfully uninstalled opencv-python-headless-4.8.0.76\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.8.0.76\n","    Uninstalling opencv-python-4.8.0.76:\n","      Successfully uninstalled opencv-python-4.8.0.76\n","Successfully installed colorama-0.4.6 immutabledict-3.0.0 opencv-python-4.7.0.72 opencv-python-headless-4.7.0.72 portalocker-2.8.2 remotezip-0.12.1 sacrebleu-2.3.1 sentencepiece-0.1.99 seqeval-1.2.2 tensorflow-model-optimization-0.7.5 tensorflow-text-2.13.0 tf-models-official-2.13.2\n"]}],"source":["!pip install remotezip tqdm opencv-python==4.7.0.72 opencv-python-headless==4.7.0.72 tf-models-official"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QImPsudoK9JI"},"outputs":[],"source":["import tqdm\n","import random\n","import pathlib\n","import itertools\n","import collections\n","\n","import cv2\n","import numpy as np\n","# import remotezip as rz\n","import zipfile as rz\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#import keras\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n","from official.projects.movinet.modeling import movinet\n","from official.projects.movinet.modeling import movinet_model"]},{"cell_type":"markdown","metadata":{"id":"2w3H4dfOPfnm"},"source":["## Load data\n","\n","The hidden cell below defines helper functions to download a slice of data from the UCF-101 dataset, and load it into a `tf.data.Dataset`. The [Loading video data tutorial](https://www.tensorflow.org/tutorials/load_data/video) provides a detailed walkthrough of this code.\n","\n","The `FrameGenerator` class at the end of the hidden block is the most important utility here. It creates an iterable object that can feed data into the TensorFlow data pipeline. Specifically, this class contains a Python generator that loads the video frames along with its encoded label. The generator (`__call__`) function yields the frame array produced by `frames_from_video_file` and a one-hot encoded vector of the label associated with the set of frames.\n","\n"]},{"cell_type":"code","source":["model_id = 'a0'\n","batch_size = 16\n","num_frames = 50\n","resolution = 360\n","num_epochs = 3\n","\n","tf.keras.backend.clear_session()\n","\n","backbone = movinet.Movinet(model_id=model_id)\n","backbone.trainable = False\n","\n","# Set num_classes=600 to load the pre-trained weights from the original model\n","model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n","model.build([None, None, None, None, 3])\n","\n","# Load pre-trained weights\n","!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n","!tar -xvf movinet_a0_base.tar.gz\n","\n","# !wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a3_base.tar.gz -O movinet_a3_base.tar.gz -q\n","#!tar -xvf movinet_a3_base.tar.gz\n","\n","#!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a2_base.tar.gz -O movinet_a2_base.tar.gz -q\n","#!tar -xvf movinet_a2_base.tar.gz\n","\n","checkpoint_dir = f'movinet_{model_id}_base'\n","checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n","checkpoint = tf.train.Checkpoint(model=model)\n","status = checkpoint.restore(checkpoint_path)\n","status.assert_existing_objects_matched()\n"],"metadata":{"id":"qLgalxA0t5P0","executionInfo":{"status":"ok","timestamp":1695035015063,"user_tz":-480,"elapsed":8563,"user":{"displayName":"gaya wood","userId":"07038287642761661483"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b3637d5-83f3-4a27-bb84-960d59d34b35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["movinet_a0_base/\n","movinet_a0_base/checkpoint\n","movinet_a0_base/ckpt-1.data-00000-of-00001\n","movinet_a0_base/ckpt-1.index\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7905e95b9b40>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# 建立分类器和模型训练\n","def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n","  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n","  model = movinet_model.MovinetClassifier(\n","      backbone=backbone,\n","      num_classes=num_classes)\n","  model.build([batch_size, num_frames, resolution, resolution, 3])\n","\n","  return model\n","\n","model = build_classifier(batch_size, num_frames, resolution, backbone, 2)\n","\n","loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n","\n","model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"],"metadata":{"id":"36iyGjtk2yt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","\n","def create_dataset(video_paths, labels, num_frames, image_size, batch_size):\n","      # 创建一个生成器\n","    def generator():\n","        for video_path, label in zip(video_paths, labels):\n","            yield extract_frames_from_video(video_path, num_frames, image_size, label)\n","\n","    # 从生成器创建数据集\n","    dataset = tf.data.Dataset.from_generator(\n","        generator,\n","        output_signature=(\n","            tf.TensorSpec(shape=(num_frames, image_size[0], image_size[1], 3), dtype=tf.uint8),\n","            tf.TensorSpec(shape=(), dtype=tf.int32)\n","        )\n","    )\n","\n","    # 打乱数据集并进行批处理\n","    dataset = dataset.shuffle(buffer_size=len(video_paths))\n","    dataset = dataset.batch(batch_size)\n","\n","    return dataset\n","\n","\n","def extract_frames_from_video(video_path, num_frames, image_size, label):\n","    # 读取视频文件\n","    cap = cv2.VideoCapture(str(video_path))\n","\n","    # 检查视频是否成功打开\n","    if not cap.isOpened():\n","        raise ValueError(f\"Failed to open video: {video_path}\")\n","\n","    # 逐帧读取视频\n","    frames = []\n","    while len(frames) < num_frames:\n","        ret, frame = cap.read()\n","        #if not ret:\n","        #    break\n","        #frame = cv2.resize(frame, image_size)\n","        # 改为如下：\n","        if not ret:\n","            # 视频帧数不足，重复最后一帧\n","            if len(frames) > 0:\n","                frame = frames[-1]\n","            # 视频中没有帧，使用零帧\n","            else:\n","                frame = np.zeros((image_size[1], image_size[0], 3), dtype=np.uint8)\n","        else:\n","            frame = cv2.resize(frame, image_size)\n","        frames.append(frame)\n","\n","    # 关闭视频文件\n","    cap.release()\n","\n","    # 将帧数据和标签返回\n","    return np.array(frames), label\n"],"metadata":{"id":"xP0A87elIbt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import pathlib\n","\n","def load_and_preprocess_video_data(data_root, num_frames, image_size, batch_size, train_ratio=0.8, shuffle=True):\n","    # 获取所有视频文件的路径\n","    video_paths = list(data_root.glob('*/*.mp4'))\n","\n","    # 创建标签字典，用于将子文件夹的名称映射到类别标签\n","    # label_to_index = dict((name, index) for index, name in enumerate(sorted(data_root.glob('*'))))\n","    # 创建标签字典，用于将子文件夹的名称映射到类别标签\n","    label_to_index = {name.name: index for index, name in enumerate(sorted(data_root.glob('*')))}\n","\n","    # 将视频路径和标签创建成列表\n","    all_video_paths = []\n","    all_video_labels = []\n","\n","    for video_path in video_paths:\n","        # 获取视频的类别标签\n","        label = label_to_index[video_path.parent.name]\n","        all_video_paths.append(str(video_path))\n","        all_video_labels.append(label)\n","\n","    # 将路径和标签转换为NumPy数组\n","    all_video_paths = np.array(all_video_paths)\n","    all_video_labels = np.array(all_video_labels)\n","\n","    # 将数据集分为训练集和测试集\n","    num_samples = len(all_video_paths)\n","    num_train_samples = int(train_ratio * num_samples)\n","\n","    if shuffle:\n","        indices = np.arange(num_samples)\n","        np.random.shuffle(indices)\n","        all_video_paths = all_video_paths[indices]\n","        all_video_labels = all_video_labels[indices]\n","\n","    train_video_paths = all_video_paths[:num_train_samples]\n","    train_video_labels = all_video_labels[:num_train_samples]\n","\n","    test_video_paths = all_video_paths[num_train_samples:]\n","    test_video_labels = all_video_labels[num_train_samples:]\n","\n","    # 构建训练数据集和测试数据集\n","    train_ds = create_dataset(train_video_paths, train_video_labels, num_frames, image_size, batch_size)\n","    test_ds = create_dataset(test_video_paths, test_video_labels, num_frames, image_size, batch_size)\n","\n","    return train_ds, test_ds\n"],"metadata":{"id":"a2IC5p9wA4s5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 数据集处理，输入一个视频数据集，安装8:2得到训练和测试数据集\n","fall_videos = pathlib.Path('./drive/MyDrive/dataFall')\n","\n","image_size=(360, 360)\n","train_ds, test_ds = load_and_preprocess_video_data(fall_videos, num_frames, image_size, batch_size)\n"],"metadata":{"id":"kVtYC9MoDi2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 定义输入形状参数\n","input_shape = [batch_size, num_frames, resolution, resolution, 3]\n","\n","# 在模型上调用 build 方法\n","model.build(input_shape)"],"metadata":{"id":"_SnZ_NexX_K-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 运行和验证模型\n","results = model.fit(train_ds,\n","                    validation_data=test_ds,\n","                    epochs=num_epochs,\n","                    validation_freq=1,\n","                    verbose=1)\n","\n","model.evaluate(test_ds, return_dict=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KmKmq1lQ8HoU","executionInfo":{"status":"ok","timestamp":1695035263346,"user_tz":-480,"elapsed":218856,"user":{"displayName":"gaya wood","userId":"07038287642761661483"}},"outputId":"fcc13304-12e6-46ec-a58b-9ff4c5b4c231"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","7/7 [==============================] - 134s 12s/step - loss: 14832.0703 - accuracy: 0.5727 - val_loss: 14222.6582 - val_accuracy: 0.6071\n","Epoch 2/3\n","7/7 [==============================] - 27s 3s/step - loss: 1931.3817 - accuracy: 0.7273 - val_loss: 3795.0156 - val_accuracy: 0.8571\n","Epoch 3/3\n","7/7 [==============================] - 26s 3s/step - loss: 2104.1777 - accuracy: 0.7909 - val_loss: 2162.1282 - val_accuracy: 0.8929\n","2/2 [==============================] - 5s 1s/step - loss: 2162.1282 - accuracy: 0.8929\n"]},{"output_type":"execute_result","data":{"text/plain":["{'loss': 2162.128173828125, 'accuracy': 0.8928571343421936}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# 保全当前模型，准备为后续推理使用\n","model.save('./drive/MyDrive/get16_movinet_a0_base')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJlkh1d4uNWD","executionInfo":{"status":"ok","timestamp":1695035391717,"user_tz":-480,"elapsed":53573,"user":{"displayName":"gaya wood","userId":"07038287642761661483"}},"outputId":"7e34aace-8b3d-4a65-c0c7-db8e007fd5ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79067d32cee0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79067d5eaf80>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79067d36b6a0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79069171d870>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79069176b6a0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79069161f8b0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7906916a6ad0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x790691537490>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7906915ba4d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79069144a620>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7906914df670>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79069136ef20>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7906912350c0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7906912ccfd0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x79069116e0e0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7905eaa4c760>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.TemporalSoftmaxPool object at 0x79067d132710>, because it is not built.\n"]}]},{"cell_type":"code","source":["# 安装预测集合\n","def video_to_input(video_path, num_frames, frame_size):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","\n","    while len(frames) < num_frames:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, frame_size)\n","        frames.append(frame)\n","\n","    cap.release()\n","\n","    # 将帧堆叠在一起并添加一个批次维度\n","    input = np.stack(frames, axis=0)[np.newaxis, ...]\n","\n","    return input\n","\n","def handle_predictions(predictions):\n","    predicted_class = np.argmax(predictions[0])\n","    print(f'The predicted class is {predicted_class}.')\n","\n","# 加载模型进行预测\n","loaded_model = tf.keras.models.load_model('./drive/MyDrive/get16_movinet_a0_base')\n","\n","# 选择预测数据集文件夹\n","fall_pre_videos = pathlib.Path('./drive/MyDrive/testDataFall/Fall')\n","\n","# 视频的帧数和帧的尺寸\n","num_frames = 160\n","frame_size = (360, 360)\n","\n","for video_path in fall_pre_videos.glob('*.mp4'):\n","    # 将视频转换为模型的输入\n","    print(f'Processing video: {video_path.name}')  # 打印文件名\n","\n","    input = video_to_input(str(video_path), num_frames, frame_size)\n","\n","    # 使用模型进行预测\n","    predictions = loaded_model.predict(input)\n","\n","    # 处理模型的预测\n","    # 这取决于你的模型的输出以及你如何解释这些输出\n","    handle_predictions(predictions)  # 你需要定义这个函数\n","\n","#batch_size = 4\n","#image_size=(360, 360)\n","#pre_train_ds, pre_test_ds = load_and_preprocess_video_data(fall_pre_videos, num_frames, image_size, batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzGtDxWkw8Lm","executionInfo":{"status":"ok","timestamp":1695035666807,"user_tz":-480,"elapsed":77168,"user":{"displayName":"gaya wood","userId":"07038287642761661483"}},"outputId":"666cb60a-164b-4d55-c395-2bbf450a4c23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing video: lalaFall_1694947509_fall-29-cam1.mp4\n","1/1 [==============================] - 8s 8s/step\n","The predicted class is 0.\n","Processing video: lalaFall_1694947509_fall-06-cam0.mp4\n","1/1 [==============================] - 9s 9s/step\n","The predicted class is 1.\n","Processing video: lalaFall_1694947510_fall-10-cam1.mp4\n","1/1 [==============================] - 5s 5s/step\n","The predicted class is 1.\n","Processing video: lalaFall_1694947512_fall-07-cam0.mp4\n","1/1 [==============================] - 4s 4s/step\n","The predicted class is 0.\n","Processing video: lalaFall_1694947512_fall-09-cam0.mp4\n","1/1 [==============================] - 4s 4s/step\n","The predicted class is 0.\n","Processing video: lalaFall_1694947566_stumbled to left fall.mp4\n","1/1 [==============================] - 4s 4s/step\n","The predicted class is 1.\n","Processing video: lalaFall_1694947568_pick up something to right fall.mp4\n","1/1 [==============================] - 4s 4s/step\n","The predicted class is 0.\n","Processing video: lalaFall_1694947569_high place fall.mp4\n","1/1 [==============================] - 4s 4s/step\n","The predicted class is 0.\n","Processing video: lalaFall_1694947572_repair right shoe fall.mp4\n","1/1 [==============================] - 0s 471ms/step\n","The predicted class is 1.\n","Processing video: lalaFall_1694947573_walking to up right fall.mp4\n","1/1 [==============================] - 0s 433ms/step\n","The predicted class is 0.\n"]}]},{"cell_type":"markdown","metadata":{"id":"lxbhPqXGvc_F"},"source":["## What are MoViNets?\n","\n","As mentioned previously, [MoViNets](https://arxiv.org/abs/2103.11511) are video classification models used for streaming video or online inference in tasks, such as action recognition. Consider using MoViNets to classify your video data for action recognition.\n","\n","A 2D frame based classifier is efficient and simple to run over whole videos, or streaming one frame at a time. Because they can't take temporal context into account they have limited accuracy and may give inconsistent outputs from frame to frame.\n","\n","A simple 3D CNN uses bidirectional temporal context which can increase accuracy and temporal consistency. These networks may require more resources and because they look into the future they can't be used for streaming data.\n","\n","![Standard convolution](https://www.tensorflow.org/images/tutorials/video/standard_convolution.png)\n","\n","The MoViNet architecture uses 3D convolutions that are \"causal\" along the time axis (like `layers.Conv1D` with `padding=\"causal\"`). This gives some of the advantages of both approaches, mainly it allow for efficient streaming.\n","\n","![Causal convolution](https://www.tensorflow.org/images/tutorials/video/causal_convolution.png)\n","\n","Causal convolution ensures that the output at time *t* is computed using only inputs up to time *t*. To demonstrate how this can make streaming more efficient, start with a simpler example you may be familiar with: an RNN. The RNN passes state forward through time:\n","\n","![RNN model](https://www.tensorflow.org/images/tutorials/video/rnn_comparison.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMvDkgfFZC6a"},"outputs":[],"source":["gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n","\n","inputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n","\n","result, state = gru(inputs) # Run it all at once"]},{"cell_type":"markdown","metadata":{"id":"T7xyb5C4bTs7"},"source":["By setting the RNN's `return_sequences=True` argument you ask it to return the state at the end of the computation. This allows you to pause and then continue where you left off, to get exactly the same result:\n","\n","![States passing in RNNs](https://www.tensorflow.org/images/tutorials/video/rnn_state_passing.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bI8FOPRRXXPa"},"outputs":[],"source":["first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state\n","second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.\n","\n","print(np.allclose(result[:, :5,:], first_half))\n","print(np.allclose(result[:, 5:,:], second_half))"]},{"cell_type":"markdown","metadata":{"id":"KM3MArumY_Qk"},"source":["Causal convolutions can be used the same way, if handled with care. This technique was used in the [Fast Wavenet Generation Algorithm](https://arxiv.org/abs/1611.09482) by Le Paine et al. In the [MoVinet paper](https://arxiv.org/abs/2103.11511), the `state` is referred to as the \"Stream Buffer\".\n","\n","![States passed in causal convolution](https://www.tensorflow.org/images/tutorials/video/causal_conv_states.png)\n","\n","By passing this little bit of state forward, you can avoid recalculating the whole receptive field that shown above."]},{"cell_type":"markdown","metadata":{"id":"1UsxiPs8yA2e"},"source":["## Download a pre-trained MoViNet model\n","\n","In this section, you will:\n","\n","1. You can create a MoViNet model using the open source code provided in [`official/projects/movinet`](https://github.com/tensorflow/models/tree/master/official/projects/movinet) from TensorFlow models.\n","2. Load the pretrained weights.\n","3. Freeze the convolutional base, or all other layers except the final classifier head, to speed up fine-tuning.\n","\n","To build the model, you can start with the `a0` configuration because it is the fastest to train when benchmarked against other models. Check out the [available MoViNet models on TensorFlow Model Garden](https://github.com/tensorflow/models/blob/master/official/projects/movinet/configs/movinet.py) to find what might work for your use case."]},{"cell_type":"markdown","metadata":{"id":"BW23HVNtCXff"},"source":["To build a classifier, create a function that takes the backbone and the number of classes in a dataset. The `build_classifier` function will take the backbone and the number of classes in a dataset to build the classifier. In this case, the new classifier will take a `num_classes` outputs (10 classes for this subset of UCF101)."]},{"cell_type":"markdown","metadata":{"id":"ddQG9sYxa1Ib"},"source":["## Next steps\n","\n","Now that you have some familiarity with the MoViNet model and how to leverage various TensorFlow APIs (for example, for transfer learning), try using the code in this tutorial with your own dataset. The data does not have to be limited to video data. Volumetric data, such as MRI scans, can also be used with 3D CNNs. The NUSDAT and IMH datasets mentioned in [Brain MRI-based 3D Convolutional Neural Networks for Classification of Schizophrenia and Controls](https://arxiv.org/pdf/2003.08818.pdf) could be two such sources for MRI data.\n","\n","In particular, using the `FrameGenerator` class used in this tutorial and the other video data and classification tutorials will help you load data into your models.\n","\n","To learn more about working with video data in TensorFlow, check out the following tutorials:\n","\n","* [Load video data](https://www.tensorflow.org/tutorials/load_data/video)\n","* [Build a 3D CNN model for video classification](https://www.tensorflow.org/tutorials/video/video_classification)\n","* [MoViNet for streaming action recognition](https://www.tensorflow.org/hub/tutorials/movinet)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb","timestamp":1694144677048}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}